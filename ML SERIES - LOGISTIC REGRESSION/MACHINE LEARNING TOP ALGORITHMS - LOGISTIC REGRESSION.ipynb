{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LOGISTIC REGRESSION ALGORITHM_\n",
    "\n",
    "Logistic Regression is a _Binary linear Classifier_ which is a function of the form: $f(x) = sign(x^Tw + w_0)$, where $w \\in \\mathbb{R}^d$ and $w_0 \\in \\mathbb{R}$\n",
    "\n",
    "The pair _$(w,w_0)$_ define an `affine hyperplane` for which it is important to develop the right geometric undestanding in relation with the linear classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AFFINE HYPERPLANE:\n",
    "A hyperplane `H` can be represented by a vector **w** as follows:\n",
    "\n",
    "$H = \\{x \\in \\mathbb{R}^d | x^Tw = 0\\}$\n",
    "\n",
    "An `affine hyperplane` is a hyperplane translated using the scalar $w_0$, so we can think of a affine hyperplane as:\n",
    "\n",
    "$H = x^Tw + w_0 = 0$\n",
    "\n",
    "Where the hyperplane has been shifted by a distance $w_0 / \\|w\\|_2$ in the direction $w$, then for a given $w, w_0$ and an input _**x**_ the inequlity $x^Tw + w_0 > 0$ says that _**x**_ is on the far side of an affine hyperplane H in the direction $w$ points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Affine Hyperplane_\n",
    "![Hyperplane](.\\image\\hyperplane.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, _Logistic Regression_ can be combined to the separating hyperplane idea to probability thru a _Bayse Classifier_.\n",
    "\n",
    "A classifier $f$ with property: $f(x) = arg max_{y\\in Y}  P(Y=y | X=x)$ for $x \\in X$ is called the `Bayes Classifier`.\n",
    "\n",
    "From **Bayes Rule** we equivalently have $f(x) = arg max_{y\\in Y}  P(Y=y)P(X=x | Y=y)$. Where:\n",
    "- $P(Y=y)$ is called the _class prior_ and $P(Y=y) = \\pi_y, y \\in \\{-1,1\\}$\n",
    "- $P(X=x | Y=y)$ is called the _class conditional distribution_ of $X$\n",
    "\n",
    "If $X$ is a continuous-valued random variable $P(X=x | Y=y)$ can be replaced by the _class conditional density_ $p(x|Y=y)$.\n",
    "\n",
    "In the binary case, we declare $y=1$ if: $p(x|y=1)P(Y=1) > p(x|y=-1)P(Y=-1) \\iff ln  p(x|y=1)p(y=1)/p(x|y=-1)p(y=-1) > 0$ \n",
    "\n",
    "where: $y \\sim Benoulli(\\pi)$ and $x|y \\sim ( \\mu_y,\\sum )$\n",
    "\n",
    "The second line is refered as the `log odds`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOG ODDS AND HYPERPLANES:\n",
    "Classifying $x$ with the `log odds`\n",
    "\n",
    "$\\large L = ln \\frac{p(y = +1|x)}{p(y = -1|x)}$\n",
    "\n",
    "We notice that:\n",
    "- $L \\gg 0$: more confident $y = +1$\n",
    "- $L \\ll 0$: more confident $y = -1$\n",
    "- $L = 0$: can go either way\n",
    "\n",
    "The linear function $x^Tw + w_0$ captures these three objectives, so we can plug in the hyperplane representation for the `log odds`.\n",
    "\n",
    "$\\large L = ln \\frac{p(y = +1|x)}{p(y = -1|x)} = x^Tw + w_0$\n",
    "\n",
    "Setting $p(y = -1|x) = 1 - p(y = +1|x)$ and solving for $p(y = +1|x)$:\n",
    "\n",
    "$\\Large p(y = +1|x) = \\frac{exp^{x^Tw + w_0}}{1 + exp^{x^Tw + w_0}} = \\sigma (x^Tw + w_0)$\n",
    "\n",
    "This is called _sigmoid function_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Sigmoid Function_\n",
    "![Sigmoid](.\\image\\sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _sigmoid Function_ $\\sigma (x^Tw + w_0)$ maps $x$ to $p(y = +1|x)$.\n",
    "\n",
    "The function captures our desire to be **more confident** as we move away from the separating hyperplane, defined by the _x-axis_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL REPRESENTATION:\n",
    "Let $(x_1,y_1), ..., (x_n,y_n)$ be a set of binary labeled data with $y \\in \\{-1,+1\\}$.\n",
    "\n",
    "_**Logistic Regression**_ models each $y_i$ as independently generated, with $P(y = +1|x_i,w) = \\sigma (x_i^Tw)$\n",
    "\n",
    "This is a discriminative classifier because $x$ is not directly modeled so, the joint likelihood of $y_1,....,y_n$ can be written as:\n",
    "\n",
    "$\\large p(y_1,...,y_n|x_1,...,x_n,w) = \\prod_{i=1}^{n} \\sigma_i (x_i^Tw)$\n",
    "\n",
    "And we want to minimize over _$\\large w$_\n",
    "\n",
    "$\\large w = arg max_w \\sum_{i=1}^{n} ln \\sigma_i (x_i^Tw)$\n",
    "\n",
    "we canâ€™t directly set $\\nabla_w \\sigma_i (x_i^Tw) = 0$, however $\\nabla_w \\sigma_i (x_i^Tw)$ does tell us the **direction** in which the function is _increasing_ with $w$ and therefore we can update the model parameters thru an iterative algorithm.\n",
    "\n",
    "This is a very general method for optimizing an objective function called `gradient descent`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT DESCENT:\n",
    "\n",
    "`Gradient descent` is an optimization algorithm used to find the values of parameters of a function $f$ that minimizes a cost function. `Gradient descent` is best used when the parameters cannot be calculated analytically and must be searched for by an optimization algorithm.\n",
    "\n",
    "`Gradient descent` can be slow to run on very large datasets. Because one iteration of the gradient descent algorithm requires a prediction for each instance in the training dataset, it can take a long time when you have many instances. In this situations we can use a variation of gradient descent called `stochastic gradient descent`, where the same procedure is run but the update to the parameters is performed for each training instance, rather than at the end of the batch of all instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION BY STOCHASTIC GRADIENT DESCENT:\n",
    "We can apply `stochastic gradient descent` to the problem of finding the coefficients for the logistic regression\n",
    "model.\n",
    "\n",
    "We have training data $(x_1,y_i), ..., (x_n,y_n)$ and a step size $\\large \\eta > 0$\n",
    "\n",
    "Let's start off by assigning 0 to each coefficient:\n",
    "\n",
    "- Set $w^{(1)} = \\vec{\\mathbf{0}}$\n",
    "\n",
    "Then calculate the new coefficient values using a simple update equation:\n",
    "\n",
    "- For step $t = 1,2,....$ update\n",
    "\n",
    "$$\\large w^{t+1} = w^t + \\eta * (y - y^-) * (1 - \\sigma(x^Tw))* y^-x$$\n",
    "\n",
    "Repeat the process and update the model for each training instance in the dataset. A single iteration through the training dataset is called an epoch. It is common to repeat the stochastic gradient descent procedure for a fixed number of epochs.\n",
    "\n",
    "Ok, let's start with the implementation of `Logistic Regression` from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##IMPORTING ALL NECESSARY SUPPORT LIBRARIES\n",
    "from math import exp\n",
    "from math import inf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataset):\n",
    "    '''\n",
    "    arguments:\n",
    "        > dataset: 2-D Array where last column is the target\n",
    "    returns:\n",
    "        > data: Augmented 2-D Array with 1 + features-columns\n",
    "        > labels: 1-D Array with the target values\n",
    "        > weights: 1-D Array with model coefficients initialized\n",
    "    '''\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    for row in dataset:\n",
    "        labels.append(row.pop())\n",
    "        row.insert(0,1)\n",
    "        data.append([x for x in row])\n",
    "        \n",
    "    weights = [0 for i in range(len(data[0]))]\n",
    "        \n",
    "    return data,labels,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights):\n",
    "    pred = sum([x*w for x,w in zip(X,weights)])\n",
    "    e = exp(-pred)\n",
    "    if e == inf:\n",
    "        return 1\n",
    "    else:\n",
    "        return 1.0/(1.0 + e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(X_train,y_train,weights,*,eta=0.1,epochs=100):\n",
    "    '''\n",
    "    arguments:\n",
    "        > X_train: Augmented 2-D Array with 1 + features-columns\n",
    "        > y_train: 1-D Array with the target values\n",
    "        > weights: 1-D Array with model coefficients\n",
    "        > eta: Learning Rate\n",
    "        > epochs: Number of max iterations\n",
    "    returns:\n",
    "        > weights: 1-D Array with model coefficients updated\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        SSE = 0\n",
    "        \n",
    "        for idx, row in enumerate(X_train):\n",
    "            pred = predict(row, weights)\n",
    "            error = y_train[idx] - pred\n",
    "            SSE += error**2\n",
    "            \n",
    "            for i in range(len(weights)):\n",
    "                weights[i] = weights[i] + eta * error * (1 - pred) * pred * row[i]\n",
    "                \n",
    "            #print('epoch: {0}, SSE: {1}'.format(epoch,SSE))\n",
    "            \n",
    "    return weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify proper implementation a **toy dataset** is used to evaluate the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [[2.7810836,2.550537003,0],\n",
    "[1.465489372,2.362125076,0],\n",
    "[3.396561688,4.400293529,0],\n",
    "[1.38807019,1.850220317,0],\n",
    "[3.06407232,3.005305973,0],\n",
    "[7.627531214,2.759262235,1],\n",
    "[5.332441248,2.088626775,1],\n",
    "[6.922596716,1.77106367,1],\n",
    "[8.675418651,-0.242068655,1],\n",
    "[7.673756466,3.508563011,1]]\n",
    "\n",
    "#OUR DATASET IS THEN PRE-PROCESSED IN ORDER TO BE READY FOR OUR 'LOGISTIC REGRESSION MODEL'\n",
    "x_data, labels, weights = prepare_data(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data ready, the coefficients have to be _learned_ to run our predictions. We will run our `update_weights` function with a learning rate $\\eta = 0.3$ and a total iterations on our data equal to $100$ times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.8596443546618896, 1.5223825112460005, -2.218700210565016]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff = update_weights(x_data,labels,weights,eta=0.3,epochs=100)\n",
    "coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our new `coefficients` we can test how well our model can predict the target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected=0, Predicted=0.09240238749672108\n",
      "Expected=0, Predicted=0.020443070793394046\n",
      "Expected=0, Predicted=0.004270645838622242\n",
      "Expected=0, Predicted=0.05460100428754744\n",
      "Expected=0, Predicted=0.05402203604622585\n",
      "Expected=1, Predicted=0.9903433037486284\n",
      "Expected=1, Predicted=0.932411366865848\n",
      "Expected=1, Predicted=0.9968264836375801\n",
      "Expected=1, Predicted=0.9999974635411156\n",
      "Expected=1, Predicted=0.9542746551950381\n"
     ]
    }
   ],
   "source": [
    "for idx, row in enumerate(x_data):\n",
    "    yhat = predict(row, coeff)\n",
    "    print(\"Expected={0}, Predicted={1}\".format(labels[idx], yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _LOGISTIC REGRESSION APPLICATION_\n",
    "\n",
    "From the `UCI Machine Learning Repository` which contains data on female patients at least 21 years old of Pima Indian heritage, we will train our `logistic regression` model using stochastic gradient descent on the **pima-indians-diabetes** dataset.\n",
    "\n",
    "The dataset have 768 instances and the following 8 attributes:\n",
    "- Number of times pregnant (preg)\n",
    "- Plasma glucose concentration a 2 hours in an oral glucose tolerance test (plas)\n",
    "- Diastolic blood pressure in mm Hg (pres)\n",
    "- Triceps skin fold thickness in mm (skin)\n",
    "- 2-Hour serum insulin in mu U/ml (insu)\n",
    "- Body mass index measured as weight in kg/(height in m)^2 (mass)\n",
    "- Age in years (age)\n",
    "- Diabetes pedigree function (pedi)\n",
    "\n",
    "A particularly interesting attribute used in the study was the Diabetes Pedigree Function, pedi. It provided some data on diabetes mellitus history in relatives and the genetic relationship of those relatives to the patient. This measure of genetic influence gave us an idea of the hereditary risk one might have with the onset of diabetes mellitus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the performance of our _Classifier_ on the **pima-indians-diabetes** dataset, a Logistic Regression model from `sklearn` will be fit on the dataset and classification report for both models is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Preg</th>\n",
       "      <th>Plas</th>\n",
       "      <th>Pres</th>\n",
       "      <th>Skin</th>\n",
       "      <th>Insu</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Pedi</th>\n",
       "      <th>Age</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Preg  Plas  Pres  Skin  Insu  Mass   Pedi  Age  target\n",
       "0     6   148    72    35     0  33.6  0.627   50       1\n",
       "1     1    85    66    29     0  26.6  0.351   31       0\n",
       "2     8   183    64     0     0  23.3  0.672   32       1\n",
       "3     1    89    66    23    94  28.1  0.167   21       0\n",
       "4     0   137    40    35   168  43.1  2.288   33       1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##LOADING 'PIMA INDIANS DIABETES' DATASET\n",
    "col = ['Preg', 'Plas', 'Pres', 'Skin', 'Insu', 'Mass', 'Pedi', 'Age', 'target']\n",
    "Pima = pd.read_csv('./data/pima-indians-diabetes.csv', names = col)\n",
    "Pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      "Preg      768 non-null int64\n",
      "Plas      768 non-null int64\n",
      "Pres      768 non-null int64\n",
      "Skin      768 non-null int64\n",
      "Insu      768 non-null int64\n",
      "Mass      768 non-null float64\n",
      "Pedi      768 non-null float64\n",
      "Age       768 non-null int64\n",
      "target    768 non-null int64\n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "Pima.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is first split into `feature values` and `labels` array loaded, and each feature-column is normalized to values in the range of 0 to 1.\n",
    "Once the preprocessing is complete the dataset will be split into a `Training` & `Test` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y, weights = prepare_data(Pima.values.tolist())\n",
    "scaler = MinMaxScaler()\n",
    "X_norm = scaler.fit_transform(X_raw)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our data ready, the coefficients have to be _learned_ to run our predictions. We will run our `update_weights` function with a learning rate $\\eta = 0.05$ and a total iterations on our data equal to $10000$ times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.3154202563989514, 2.2690659261588677, -5.019016039713838, -0.3123259665800087, 0.7656450106312648, 0.017726651509694727, 1.564383609909205, 2.372423223321087]\n"
     ]
    }
   ],
   "source": [
    "logr_coef = update_weights(X_train,y_train,weights,eta=0.05,epochs=10000)\n",
    "print(logr_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATE CUSTOMIZED PREDICTIONS ARRAY\n",
    "pred = [predict(row,logr_coef) for row in X_test]\n",
    "cust_pred = np.array([1 if x >=0.5 else 0 for x in pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now an instance of `sklearn` _Logistic Regression_ model is created and fit it with the training data and an array of predictions is obtained in order to get out performance comparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\devop\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "##GET AND INSTANCIATE OF LOGISTIC REGRESSION MODEL\n",
    "sk_lr = LogisticRegression()\n",
    "sk_lr.fit(X_train, y_train)\n",
    "\n",
    "##CREATE SKLEARN PREDICTIONS ARRAY\n",
    "sk_pred = sk_lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  1.59953863,  3.64731438, -1.14311614, -0.15296794,\n",
       "         0.39461853,  2.05479615,  1.41757151,  1.24727379]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By last, a comparison on both models is performed thru a _Classification Report_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sklearn:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83       160\n",
      "         1.0       0.62      0.59      0.60        71\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       231\n",
      "   macro avg       0.72      0.71      0.72       231\n",
      "weighted avg       0.76      0.76      0.76       231\n",
      "\n",
      "Custom:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.78      0.78       160\n",
      "         1.0       0.51      0.52      0.52        71\n",
      "\n",
      "   micro avg       0.70      0.70      0.70       231\n",
      "   macro avg       0.65      0.65      0.65       231\n",
      "weighted avg       0.70      0.70      0.70       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Sklearn:\")\n",
    "print(classification_report(y_test, sk_pred))\n",
    "print(\"Custom:\")\n",
    "print(classification_report(y_test, cust_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
